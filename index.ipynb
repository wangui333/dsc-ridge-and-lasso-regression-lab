{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI9vQ36wZcJT"
      },
      "source": [
        "# Ridge and Lasso Regression - Lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGBhRmlkZcJZ"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29fVS1GSZcJc"
      },
      "source": [
        "In this lab, you'll practice your knowledge of ridge and lasso regression!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaLpbl5FZcJe"
      },
      "source": [
        "## Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRAGnGfCZcJg"
      },
      "source": [
        "In this lab you will:\n",
        "\n",
        "- Use lasso and ridge regression with scikit-learn\n",
        "- Compare and contrast lasso, ridge and non-regularized regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI4YMIhKZcJj"
      },
      "source": [
        "## Housing Prices Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCfZPGWHZcJl"
      },
      "source": [
        "We'll use this version of the Ames Housing dataset:"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-GW9X96hbdqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBkS2ME4ZcJm",
        "outputId": "12f3e9f9-5aff-401b-e29f-b28d44a567af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1460 entries, 1 to 1460\n",
            "Data columns (total 80 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   MSSubClass     1460 non-null   int64  \n",
            " 1   MSZoning       1460 non-null   object \n",
            " 2   LotFrontage    1201 non-null   float64\n",
            " 3   LotArea        1460 non-null   int64  \n",
            " 4   Street         1460 non-null   object \n",
            " 5   Alley          91 non-null     object \n",
            " 6   LotShape       1460 non-null   object \n",
            " 7   LandContour    1460 non-null   object \n",
            " 8   Utilities      1460 non-null   object \n",
            " 9   LotConfig      1460 non-null   object \n",
            " 10  LandSlope      1460 non-null   object \n",
            " 11  Neighborhood   1460 non-null   object \n",
            " 12  Condition1     1460 non-null   object \n",
            " 13  Condition2     1460 non-null   object \n",
            " 14  BldgType       1460 non-null   object \n",
            " 15  HouseStyle     1460 non-null   object \n",
            " 16  OverallQual    1460 non-null   int64  \n",
            " 17  OverallCond    1460 non-null   int64  \n",
            " 18  YearBuilt      1460 non-null   int64  \n",
            " 19  YearRemodAdd   1460 non-null   int64  \n",
            " 20  RoofStyle      1460 non-null   object \n",
            " 21  RoofMatl       1460 non-null   object \n",
            " 22  Exterior1st    1460 non-null   object \n",
            " 23  Exterior2nd    1460 non-null   object \n",
            " 24  MasVnrType     588 non-null    object \n",
            " 25  MasVnrArea     1452 non-null   float64\n",
            " 26  ExterQual      1460 non-null   object \n",
            " 27  ExterCond      1460 non-null   object \n",
            " 28  Foundation     1460 non-null   object \n",
            " 29  BsmtQual       1423 non-null   object \n",
            " 30  BsmtCond       1423 non-null   object \n",
            " 31  BsmtExposure   1422 non-null   object \n",
            " 32  BsmtFinType1   1423 non-null   object \n",
            " 33  BsmtFinSF1     1460 non-null   int64  \n",
            " 34  BsmtFinType2   1422 non-null   object \n",
            " 35  BsmtFinSF2     1460 non-null   int64  \n",
            " 36  BsmtUnfSF      1460 non-null   int64  \n",
            " 37  TotalBsmtSF    1460 non-null   int64  \n",
            " 38  Heating        1460 non-null   object \n",
            " 39  HeatingQC      1460 non-null   object \n",
            " 40  CentralAir     1460 non-null   object \n",
            " 41  Electrical     1459 non-null   object \n",
            " 42  1stFlrSF       1460 non-null   int64  \n",
            " 43  2ndFlrSF       1460 non-null   int64  \n",
            " 44  LowQualFinSF   1460 non-null   int64  \n",
            " 45  GrLivArea      1460 non-null   int64  \n",
            " 46  BsmtFullBath   1460 non-null   int64  \n",
            " 47  BsmtHalfBath   1460 non-null   int64  \n",
            " 48  FullBath       1460 non-null   int64  \n",
            " 49  HalfBath       1460 non-null   int64  \n",
            " 50  BedroomAbvGr   1460 non-null   int64  \n",
            " 51  KitchenAbvGr   1460 non-null   int64  \n",
            " 52  KitchenQual    1460 non-null   object \n",
            " 53  TotRmsAbvGrd   1460 non-null   int64  \n",
            " 54  Functional     1460 non-null   object \n",
            " 55  Fireplaces     1460 non-null   int64  \n",
            " 56  FireplaceQu    770 non-null    object \n",
            " 57  GarageType     1379 non-null   object \n",
            " 58  GarageYrBlt    1379 non-null   float64\n",
            " 59  GarageFinish   1379 non-null   object \n",
            " 60  GarageCars     1460 non-null   int64  \n",
            " 61  GarageArea     1460 non-null   int64  \n",
            " 62  GarageQual     1379 non-null   object \n",
            " 63  GarageCond     1379 non-null   object \n",
            " 64  PavedDrive     1460 non-null   object \n",
            " 65  WoodDeckSF     1460 non-null   int64  \n",
            " 66  OpenPorchSF    1460 non-null   int64  \n",
            " 67  EnclosedPorch  1460 non-null   int64  \n",
            " 68  3SsnPorch      1460 non-null   int64  \n",
            " 69  ScreenPorch    1460 non-null   int64  \n",
            " 70  PoolArea       1460 non-null   int64  \n",
            " 71  PoolQC         7 non-null      object \n",
            " 72  Fence          281 non-null    object \n",
            " 73  MiscFeature    54 non-null     object \n",
            " 74  MiscVal        1460 non-null   int64  \n",
            " 75  MoSold         1460 non-null   int64  \n",
            " 76  YrSold         1460 non-null   int64  \n",
            " 77  SaleType       1460 non-null   object \n",
            " 78  SaleCondition  1460 non-null   object \n",
            " 79  SalePrice      1460 non-null   int64  \n",
            "dtypes: float64(3), int64(34), object(43)\n",
            "memory usage: 923.9+ KB\n"
          ]
        }
      ],
      "source": [
        "# Run this cell without changes\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "df = pd.read_csv('housing_prices.csv', index_col=0)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wDfMhAc8e1QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_SNyeLIZcJp"
      },
      "source": [
        "More information about the features is available in the `data_description.txt` file in this repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD_QOreZZcJq"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "The code below:\n",
        "\n",
        "* Separates the data into `X` (predictor) and `y` (target) variables\n",
        "* Splits the data into 75-25 training-test sets, with a `random_state` of 10\n",
        "* Separates each of the `X` values into continuous vs. categorical features\n",
        "* Fills in missing values (using different strategies for continuous vs. categorical features)\n",
        "* Scales continuous features to a range of 0 to 1\n",
        "* Dummy encodes categorical features\n",
        "* Combines the preprocessed continuous and categorical features back together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "poVLSuKVZcJs"
      },
      "outputs": [],
      "source": [
        "# Run this cell without changes\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "\n",
        "# Create X and y\n",
        "y = df['SalePrice']\n",
        "X = df.drop(columns=['SalePrice'])\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)\n",
        "\n",
        "# Separate X data into continuous vs. categorical\n",
        "X_train_cont = X_train.select_dtypes(include='number')\n",
        "X_test_cont = X_test.select_dtypes(include='number')\n",
        "X_train_cat = X_train.select_dtypes(exclude='number')\n",
        "X_test_cat = X_test.select_dtypes(exclude='number')\n",
        "\n",
        "# Impute missing values using SimpleImputer, median for continuous and\n",
        "# filling in 'missing' for categorical\n",
        "impute_cont = SimpleImputer(strategy='median')\n",
        "X_train_cont = impute_cont.fit_transform(X_train_cont)\n",
        "X_test_cont = impute_cont.transform(X_test_cont)\n",
        "impute_cat = SimpleImputer(strategy='constant', fill_value='missing')\n",
        "X_train_cat = impute_cat.fit_transform(X_train_cat)\n",
        "X_test_cat = impute_cat.transform(X_test_cat)\n",
        "\n",
        "# Scale continuous values using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train_cont = scaler.fit_transform(X_train_cont)\n",
        "X_test_cont = scaler.transform(X_test_cont)\n",
        "\n",
        "# Dummy encode categorical values using OneHotEncoder\n",
        "ohe = OneHotEncoder(handle_unknown='ignore')\n",
        "X_train_cat = ohe.fit_transform(X_train_cat)\n",
        "X_test_cat = ohe.transform(X_test_cat)\n",
        "\n",
        "# Combine everything back together\n",
        "X_train_preprocessed = np.asarray(np.concatenate([X_train_cont, X_train_cat.todense()], axis=1))\n",
        "X_test_preprocessed = np.asarray(np.concatenate([X_test_cont, X_test_cat.todense()], axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxmn4-70ZcJu"
      },
      "source": [
        "## Linear Regression Model\n",
        "\n",
        "Let's use this data to build a first naive linear regression model. Fit the model on the training data (`X_train_preprocessed`), then compute the R-Squared and the MSE for both the training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGH_GszWZcJv",
        "outputId": "4dc8a4c5-337d-4aaa-de0c-8864543e2817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training R-Squared: 0.9358295516109549\n",
            "test R-Squared: 0.8574960937151468\n"
          ]
        }
      ],
      "source": [
        "# Replace None with appropriate code\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Fit the model\n",
        "linreg = LinearRegression()\n",
        "linreg.fit(X_train_preprocessed, y_train)\n",
        "\n",
        "# Compute R-Squared and MSE for training and test sets\n",
        "\n",
        "# Print R2 and MSE for training and test sets\n",
        "print(f'training R-Squared: {linreg.score(X_train_preprocessed, y_train)}')\n",
        "print(f'test R-Squared: {linreg.score(X_test_preprocessed, y_test)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IKL_X1zZcJw"
      },
      "source": [
        "Notice the severe overfitting above; our training R-Squared is very high, but the test R-Squared is negative! Similarly, the scale of the test MSE is orders of magnitude higher than that of the training MSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbP1PbHvZcJx"
      },
      "source": [
        "## Ridge and Lasso Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWMtXSLhZcJy"
      },
      "source": [
        "Use all the data (scaled features and dummy categorical variables, `X_train_preprocessed`) to build some models with regularization - two each for lasso and ridge regression. Each time, look at R-Squared and MSE.\n",
        "\n",
        "Remember that you can use the scikit-learn documentation if you don't remember how to import or use these classes:\n",
        "\n",
        "* [`Lasso` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
        "* [`Ridge` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5m5ed_MZcJz"
      },
      "source": [
        "### Lasso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3nLWD-6ZcJ0"
      },
      "source": [
        "#### With default hyperparameters (`alpha` = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD1SNZHrZcJ1",
        "outputId": "2661021c-5c11-4574-cb84-c1f1c26e9108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lasso MSE: 714345171.6235408\n",
            "Training r^2: 0.9358284149028193\n",
            "Test r^2:     0.8880369206568858\n",
            "Training MSE: 403062981.04981583\n",
            "Test MSE:     714345171.6235408\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "from  sklearn.linear_model import Ridge, Lasso\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_train_preprocessed, y_train)\n",
        "lasso_pred = lasso.predict(X_test_preprocessed)\n",
        "print(\"Lasso MSE:\", mean_squared_error(y_test, lasso_pred))\n",
        "print('Training r^2:', lasso.score(X_train_preprocessed, y_train))\n",
        "print('Test r^2:    ', lasso.score(X_test_preprocessed, y_test))\n",
        "print('Training MSE:', mean_squared_error(y_train, lasso.predict(X_train_preprocessed)))\n",
        "print('Test MSE:    ', mean_squared_error(y_test, lasso.predict(X_test_preprocessed)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP-2sB_gZcJ1"
      },
      "source": [
        "#### With a higher regularization hyperparameter (`alpha` = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pladY6I9ZcJ2"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61XEvifLZcJ3"
      },
      "source": [
        "## Ridge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaPW7nZaZcJ4"
      },
      "source": [
        "#### With default hyperparameters (`alpha` = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShaztMdeZcJ5"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyxFbh9TZcJ6"
      },
      "source": [
        "#### With higher regularization hyperparameter (`alpha` = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fORmQVrAZcJ7"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdZWdzjHZcJ8"
      },
      "source": [
        "## Comparing the Metrics    \n",
        "\n",
        "Which model seems best, based on the metrics?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4eV8QIsZcJ9"
      },
      "outputs": [],
      "source": [
        "# Write your conclusions here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvEkqsXZZcJ-"
      },
      "source": [
        "<details>\n",
        "    <summary style=\"cursor: pointer\"><b>Answer (click to reveal)</b></summary>\n",
        "\n",
        "In terms of both R-Squared and MSE, the `Lasso` model with `alpha`=10 has the best metric results.\n",
        "\n",
        "(Remember that better R-Squared is higher, whereas better MSE is lower.)\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFaAnaWSZcJ_"
      },
      "source": [
        "## Comparing the Parameters\n",
        "\n",
        "Compare the number of parameter estimates that are (very close to) 0 for the `Ridge` and `Lasso` models with `alpha`=10.\n",
        "\n",
        "Use 10**(-10) as an estimate that is very close to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuZSqf6LZcKA"
      },
      "outputs": [],
      "source": [
        "# Number of Ridge params almost zero\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXBh-AnOZcKC"
      },
      "outputs": [],
      "source": [
        "# Number of Lasso params almost zero\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0clVAz8CZcKE"
      },
      "outputs": [],
      "source": [
        "# Compare and interpret these results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqjbdpGKZcKH"
      },
      "source": [
        "<details>\n",
        "    <summary style=\"cursor: pointer\"><b>Answer (click to reveal)</b></summary>\n",
        "\n",
        "The ridge model did not penalize any coefficients to 0, while the lasso model removed about 1/4 of the coefficients. The lasso model essentially performed variable selection for us, and got the best metrics as a result!\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnPrxHfcZcKK"
      },
      "source": [
        "## Finding an Optimal Alpha\n",
        "\n",
        "Earlier we tested two values of `alpha` to see how it affected our MSE and the value of our coefficients. We could continue to guess values of `alpha` for our ridge or lasso regression one at a time to see which values minimize our loss, or we can test a range of values and pick the alpha which minimizes our MSE. Here is an example of how we would do this:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "5rv8esS6ZcKL",
        "outputId": "78441f4f-c3a2-4dce-a7b0-2dcdd208efd2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Lasso' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3600388566.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mlasso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mlasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_preprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Lasso' is not defined"
          ]
        }
      ],
      "source": [
        "# Run this cell without changes\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "train_mse = []\n",
        "test_mse = []\n",
        "alphas = np.linspace(0, 200, num=50)\n",
        "\n",
        "for alpha in alphas:\n",
        "    lasso = Lasso(alpha=alpha)\n",
        "    lasso.fit(X_train_preprocessed, y_train)\n",
        "\n",
        "    train_preds = lasso.predict(X_train_preprocessed)\n",
        "    train_mse.append(mean_squared_error(y_train, train_preds))\n",
        "\n",
        "    test_preds = lasso.predict(X_test_preprocessed)\n",
        "    test_mse.append(mean_squared_error(y_test, test_preds))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alphas, train_mse, label='Train')\n",
        "ax.plot(alphas, test_mse, label='Test')\n",
        "ax.set_xlabel('alpha')\n",
        "ax.set_ylabel('MSE')\n",
        "\n",
        "# np.argmin() returns the index of the minimum value in a list\n",
        "optimal_alpha = alphas[np.argmin(test_mse)]\n",
        "\n",
        "# Add a vertical line where the test MSE is minimized\n",
        "ax.axvline(optimal_alpha, color='black', linestyle='--')\n",
        "ax.legend();\n",
        "\n",
        "print(f'Optimal Alpha Value: {int(optimal_alpha)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5vJvfsbZcKN"
      },
      "source": [
        "Take a look at this graph of our training and test MSE against `alpha`. Try to explain to yourself why the shapes of the training and test curves are this way. Make sure to think about what `alpha` represents and how it relates to overfitting vs underfitting.\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "    <summary style=\"cursor: pointer\"><b>Answer (click to reveal)</b></summary>\n",
        "\n",
        "For `alpha` values below 28, the model is overfitting. As `alpha` increases up to 28, the MSE for the training data increases and MSE for the test data decreases, indicating that we are reducing overfitting.\n",
        "\n",
        "For `alpha` values above 28, the model is starting to underfit. You can tell because _both_ the train and the test MSE values are increasing.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD9sFqPrZcKO"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Well done! You now know how to build lasso and ridge regression models, use them for feature selection and find an optimal value for `alpha`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (learn-env)",
      "language": "python",
      "name": "learn-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}